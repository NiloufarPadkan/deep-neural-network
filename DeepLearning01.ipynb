# **پروژه اول درس یادگیری عمیق**

---
 fateme padkan 
970122681002


---
###  ◼ Preprocessing  
import os
import pandas as pd
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

!gdown --id 1-Zyp-JP3f9QhPKaErBkPPFNKaPS1v74u

categorical_attr = ['gender', 'NationalITy', 'PlaceofBirth', 'StageID', 'GradeID', 'SectionID', 'Topic', 'Semester', 'Relation', 'ParentAnsweringSurvey', 'ParentschoolSatisfaction', 'StudentAbsenceDays', 'Class']

df = pd.read_csv('/content/Dataset.csv')
df.head()

print(df.shape)
# Converting Categorical values to scaler values
le = LabelEncoder()
df[categorical_attr] = df[categorical_attr].apply(le.fit_transform, axis=0)
df.head()
# X: Features, y: Classes
X = np.array(df.iloc[:, :-1])
y = np.array(df['Class'])

### ◼ lables must  be 0 or 1

temp=np.amax(X, axis=0)
X= X/temp

for i in range(0,len(y)) : #H,M pass    L=fail
  
  if y[i]>0 :
     y[i]=1
  else :
     y[i]=0

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=12)
X_train = X_train.T
X_val = X_val.T
y_train = y_train.T.reshape(1, len(y_train))
y_val = y_val.T.reshape(1, len(y_val))
print('Number of dataset: ', len(X))
print('Number of train set: ', len(X_train))
print('Number of validation set: ', len(X_val))
### ◼ math utils


def sigmoid(Z):

    A = 1/(1+np.exp(-Z))  
    return A



def sigmoid_backward(dA, cache):
   
    
    Z = cache
    
    s = 1/(1+np.exp(-Z))
    dZ = dA * s * (1-s)
    
    
    return dZ

def relu(Z):
    A=np.where(np.asarray(Z) > 0, Z, 0)
    
    return A
def relu_backward(dA, cache):
   
    Z = cache
    dZ = np.array(dA, copy=True) 
    
    dZ[Z <= 0] = 0
        
    return dZ



### ◼ *in predict function :* 


⭕ **converting output of forward function to 0,1** 


> input:

*  shape : shape of train or validation data . 

 ✅    we need it for making a predict matrix which is [1][shape[1]]



*   Y : train or validation Y

*    output: output of forward prpopagation 





> output:

* p : predict matrix 













def predict( shape,Y, output):
    p = np.zeros((1,shape[1]))
    for i in range(0, output.shape[1]):
        if output[0,i] > 0.5:
            p[0][i] = 1
        else:
            p[0][i] = 0
                   
    return p
### ◼ forward Function for l layer:

 

> input:

*  X: data 
*  n:number of layers
*  parameters : w and b (output of initilizer) 



other params :

*  A_prev: X or A (activation) of a prev layer
*  w : weight  
*  b : bias
* activationType : relu or tanh or sigmoid 

* A :  the output of the activation function 

* A_W_B_Z_dict: a dictionary with A,W,b,Z




> 
1.  for each layer z :  z(l)=W(l)A(l-1)+b(l)







def forwardFunction(X,n, parameters):
    caches = []
    A = X
    L = n-1    
    for l in range(1, L):
        previousA = A
        W=parameters['W' + str(l)]
        b=parameters['b' + str(l)]
        A_W_b_dict = (previousA, W, b)
        Z = np.dot(W, previousA) + b #linear part
        A = relu(Z)    #non linear      
        A_W_B_Z_dict = (A_W_b_dict, Z)
        caches.append(A_W_B_Z_dict)


    W=parameters['W' + str(L)]
    b=parameters['b' + str(L)]
    Z = np.dot(W, A) + b
    A_W_b_dict = (A, W, b)
    AL = sigmoid(Z)
    A_W_B_Z_dict = (A_W_b_dict, Z)

  
    caches.append(A_W_B_Z_dict)

    return AL, caches

### ◼ compute cost function 


 
*compute_cost :*

> input:

*  AL  : probability vector  and from forward function 
*  Y


> output:

* cost 




def compute_cost(AL, Y):
    m = Y.shape[1]
    cost = (-1/m) * (np.dot(Y, np.log(AL).T) + np.dot((1-Y), np.log(1-AL).T))
    return cost
### ◼ backward propagation


def backwardFunction(AL, Y, caches):

    L = len(caches) 
    m = AL.shape[1]
    Y = Y.reshape(AL.shape) 
    gradiants = {}  #gradiant of the loss function


    current_cache = caches[L-1]  
    AWB, Z = current_cache

    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))  ## derivative of cost with respect to AL
    dZ = sigmoid_backward(dAL, Z)  #dz(l)=dA(l)*g'(z(l))

    A_prev, W, b = AWB
    m = A_prev.shape[1]
    dW = (1/m) * np.dot(dZ, A_prev.T)
    db = (1/m) * np.sum(dZ, axis=1, keepdims=True)
    dA_prev = np.dot(W.T, dZ)        

    gradiants["dA" + str(L-1)]=dA_prev
    gradiants["dW" + str(L)]=dW
    gradiants["db" + str(L)]=db         
 

    for l in reversed(range(L-1)):

        current_cache = caches[l]
        
        AWB, Z = current_cache

        dZ = relu_backward( gradiants["dA" + str(l + 1)], Z)

        A_prev, W, b = AWB
        m = A_prev.shape[1]
        dW = (1/m) * np.dot(dZ, A_prev.T)
        db = (1/m) * np.sum(dZ, axis=1, keepdims=True)
        dA_prev = np.dot(W.T, dZ)

        gradiants["dA" + str(l)] = dA_prev
        gradiants["dW" + str(l + 1)] = dW
        gradiants["db" + str(l + 1)] = db

    return gradiants
### ◼ update the parameters of the model, using gradient descent


*  w(l)=w(l)- lr_rate*dw(l)


*  b(l)=b(l)- lr_rate*db(l)

def update_parameters(parameters,dims, grads, learning_rate):

    L = len(dims)-1
  
    for l in range(L):
        parameters["W" + str(l+1)] = parameters["W" + str(l+1)] -  learning_rate * grads["dW" + str(l+1)]
        parameters["b" + str(l+1)] = parameters["b" + str(l+1)] - learning_rate * grads["db" + str(l+1)]
    return parameters

### ◼ function for implementring model


### *in initializer function :* 

⭕ **first inititalize nueral network ** 

⬜ random initializer for random w and b  and zero initializer for zero w and b

* w_b_dict : a dictionary with W and b

* layersNum: number of layers 





def implementNetwork(X, Y ,X_val, Y_val, layers_dims, lr_rate = 0.1,randomInit=1, itersNum = 1000):#lr was 0.009

    np.random.seed(1)
    itersValues=[]
    dims=layers_dims
    trainShape=X.shape
    validatonShape=X_val.shape
    
    n = len(layers_dims) 
    print(n,"layer")

    
    w_b_dict = {}

    if randomInit==1 :

      np.random.seed(3)

      for l in range(1, len(dims)):  # from 1 to number of layers
          w_b_dict['W' + str(l)] = np.random.randn(dims[l],
                                                    dims[l-1]) * 0.01
          w_b_dict["b" + str(l)] = np.random.randn(dims[l ], 1) * 0.01

      layersNum=len(dims)
      parameters=w_b_dict;

    else :


      for l in range(1, len(dims)):  # from 1 to number of layers
        w_b_dict['W' + str(l)] =np.zeros((dims[l], dims[l-1]))
        w_b_dict["b" + str(l)] = np.zeros((dims[l ], 1)) 

      layersNum=len(dims)
      parameters=w_b_dict;
     
    trainLoss=[]
    validationLoss=[]
    trainAccu=[]
    validationAccu=[]
    for i in range(0, itersNum):

       
        AL,cachedValues = forwardFunction(X,n, parameters)
     
        cost = compute_cost(AL, Y)
       
        grads = backwardFunction(AL, Y,cachedValues)
        
        parameters = update_parameters(parameters,dims, grads, lr_rate)
      


        if   i % 100 == 0:

           

            validationOutput,chaches = forwardFunction(X_val,n, parameters)

            predict_train =predict(trainShape,Y,AL)
            predict_val = predict(validatonShape,Y_val,validationOutput)

             
            computeTrainLoss=(compute_cost(AL, Y)[0])
            trainLoss.append(computeTrainLoss[0])
            
            computeValidationLoss=(compute_cost(validationOutput, Y_val)[0])
            validationLoss.append(computeValidationLoss[0])

    
            #percentage of true predictions
            trainAccu.append(np.sum((predict_train == Y)/trainShape[1]))
            validationAccu.append(np.sum((predict_val == Y_val)/validatonShape[1]))

            itersValues.append(i)


        
  
   
    print("trainLoss",trainLoss)
    print("validationLoss",validationLoss)
    print("trainAccuracy",validationLoss)
    print("validationAccuracy",validationAccu)

    i=trainLoss.index(np.sort(trainLoss)[0])
    print("\n \nfor best trainLoss \ntrainloss ",trainLoss[i])
    print("trainAccuracy",trainAccu[i],"\n")

    i=validationLoss.index(np.sort(validationLoss)[0])
    print("\n \nfor best validation loss \nvalidation loss ",validationLoss[i])
    print("validation Accuracy",validationAccu[i],"\n")

    i=trainAccu.index(np.sort(trainAccu)[-1])
    print("\n \nfor best trainAccuracy  \ntrainAccuracy",trainAccu[i])
    print("trainLoss",trainLoss[i],"\n")

    i=validationAccu.index(np.sort(validationAccu)[-1])
    print("\n \nfor best validationAccuracy  \nvalidationAccu",validationAccu[i])
    print("trainLoss",trainLoss[i],"\n")

    

    
    
    
    return itersValues,trainLoss,validationLoss,trainAccu,validationAccu
### ◼ drawing graph function
def draw_pot(title,iters,trainLoss,validationLoss,trainAccu,validationAccu):
    fig, axs = plt.subplots(2, 2, figsize=(20, 10))

    x = np.squeeze(iters)
    axs[0, 0].plot(x,np.squeeze(trainAccu))
    axs[0, 0].set_title('Accuracy train')
    axs[0, 1].plot(x,trainLoss, 'tab:orange')
    axs[0, 1].set_title('loss train')
    axs[1, 0].plot(x, np.squeeze(validationAccu), 'tab:green')
    axs[1, 0].set_title('Accuracy val')
    axs[1, 1].plot(x, validationLoss, 'tab:red')
    axs[1, 1].set_title('loss val')
    plt.suptitle(title, fontsize=20, fontweight='bold')
    for ax in axs.flat:
        ax.set(xlabel='Itration', ylabel='Percentage', ylim=[0, 1])

    # Hide x labels and tick labels for top plots and y ticks for right plots.
    # for ax in axs.flat:
    #     ax.label_outer()
### ◼ call implement model function
### 2 layer random initialization
iters,trainLoss,validationLoss,trainAccu,validationAccu = implementNetwork(X_train, y_train,X_val,y_val, [16, 1], itersNum = 1000)

draw_pot("2 layer random initialization",iters,trainLoss,validationLoss,trainAccu,validationAccu)
iters,trainLoss,validationLoss,trainAccu,validationAccu = implementNetwork(X_train, y_train,X_val,y_val, [16,25, 1], itersNum = 1000)

draw_pot("3 layer random initialization",iters,trainLoss,validationLoss,trainAccu,validationAccu)

iters,trainLoss,validationLoss,trainAccu,validationAccu = implementNetwork(X_train, y_train,X_val,y_val, [16,25,52,26, 1], itersNum = 1000)

draw_pot("5 layer random initialization",iters,trainLoss,validationLoss,trainAccu,validationAccu)

iters,trainLoss,validationLoss,trainAccu,validationAccu = implementNetwork(X_train, y_train,X_val,y_val, [16, 1],randomInit=0, itersNum = 1000)

draw_pot("2 layer zero initialization",iters,trainLoss,validationLoss,trainAccu,validationAccu)

iters,trainLoss,validationLoss,trainAccu,validationAccu = implementNetwork(X_train, y_train,X_val,y_val, [16,25, 1],randomInit=0, itersNum = 1000)

draw_pot("3 layer zero initialization",iters,trainLoss,validationLoss,trainAccu,validationAccu)

iters,trainLoss,validationLoss,trainAccu,validationAccu = implementNetwork(X_train, y_train,X_val,y_val, [16,25,52,26, 1],randomInit=0, itersNum = 1000)

draw_pot("5 layer zero initialization",iters,trainLoss,validationLoss,trainAccu,validationAccu)

